# -*- coding: utf-8 -*-
"""mybert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F4sJfvOdStAP-tURWPGb3zxCOTbgy_3G
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

import pandas as pd
import math
import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

from tensorflow.keras import layers
from tensorflow.keras.layers import Input, GRU, Dense, Dropout
# git clone https://github.com/google-research/bert.git

import bert
from bert import tokenization
from bert import modeling
# from bert import extract_features
# from bert import run_classifier
from matplotlib import pyplot as plt


df_data = pd.read_csv('/home/ubuntu/ner_dataset.csv')
print(df_data.head(10))
df_data = df_data.iloc[:30000]

"""Get the token of words form vocab, convert the token to number"""

class SentenceGetter(object):

    def __init__(self, data):
        self.n_sent = 1
        self.data = data
        self.empty = False
        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s["Word"].values.tolist(),
                                                           s["POS"].values.tolist(),
                                                           s["Tag"].values.tolist())]
        self.grouped = self.data.groupby("Sentence #").apply(agg_func)
        self.sentences = [s for s in self.grouped]

    def get_next(self):
        try:
            s = self.grouped["Sentence: {}".format(self.n_sent)]
            self.n_sent += 1
            return s
        except:
            return None

getter = SentenceGetter(df_data)
sentences = [[s[0] for s in sent] for sent in getter.sentences]
labels = [[s[2] for s in sent] for sent in getter.sentences]

print(sentences[0])
print(labels[0])

vocab='/home/ubuntu/bert/vocab.txt'
tokenizer = tokenization.FullTokenizer(vocab,do_lower_case=False)

tokenizer.tokenize('demonstrators') #'demonstrators'

txt_token = []
token_label = [] # sentences num * word in sentence
for sentence, label1 in zip(sentences, labels):
  word_list, label_list = ['[CLS]'], ['[CLS]']
  for word, label in zip(sentence, label1):
    token_list=tokenizer.tokenize(word)
    for i, j in enumerate(token_list):
      word_list.append(j)
      if i == 0:
        label_list.append(label)
      else:
        label_list.append('X') #explore
  word_list.append('[SEP]')
  label_list.append('[SEP]')
  txt_token.append(word_list)
  token_label.append(label_list)

print(txt_token[0])
print(token_label[0])

input_ids = []
for i in txt_token:
  temp = tokenizer.convert_tokens_to_ids(i)
  input_ids.append(temp)
print('input_id 0',input_ids[0])
print(tokenizer.convert_tokens_to_ids(','))

max_len = max([len(x) for x in input_ids])+1 #explore
print(max_len)

input_ids = pad_sequences(input_ids, maxlen=max_len,dtype='long',padding='post', truncating='post')
print(input_ids[0])

"""Convert the tag label to numeracal label"""

tag2id = {}
j=1
for i in df_data['Tag'].unique():
  tag2id[i]=j
  j+=1
tag2id['X']=17
tag2id['[CLS]']=18
tag2id['[SEP]']=19
tag2id

output_ids=[]
for i in token_label:
  temp=[]
  for j in i:
    temp.append(tag2id[j])
  output_ids.append(temp)
print(output_ids[0])

output_ids = pad_sequences(output_ids, maxlen=max_len,dtype='long',padding='post', truncating='post')
print(output_ids[0])
len(output_ids[0])

# split to train, val and test data
val_inputs, val_ouputs = input_ids[-1000:], output_ids[-1000:]
tr_inputs, test_inputs, tr_ouputs, test_ouputs = train_test_split(input_ids[:-200], output_ids[:-200], test_size=0.2)

# For test data, record how many real words in each sentence
temp = np.sign(test_ouputs)
print(temp.shape)
print(test_ouputs.shape)
print(temp[0])
real_length = np.sum(temp,axis=1)
print(real_length.shape)

"""Create the mask id and segment_id"""

# The mask id is 1 for real input words, is 0 for padding words
input_mask = [[0 if j==0 else 1 for j in i] for i in tr_inputs]
print('inputmask 0',input_mask[0])
print(len(input_mask))

val_mask = [[0 if j==0 else 1 for j in i] for i in val_inputs]
print('valinput mask', val_mask[0])

test_mask = [[0 if j==0 else 1 for j in i] for i in test_inputs]

# The name entity recognition task use one sentence at a time, actually don't need
# segment_id to distinguish two sentence in one input (0 for first sentence 1 for second sentence). So set all segment_id to 0
segment_id = np.zeros(tr_inputs.shape)
print('train segmentid shape',segment_id.shape)

val_segment_id = np.zeros(val_inputs.shape)

test_segment_id = np.zeros(test_inputs.shape)

"""Create the model"""

temp = tokenization.load_vocab(vocab)
vocab_size = len(temp)
print(vocab_size)

bert_config = modeling.BertConfig.from_json_file('/home/ubuntu/bert/bert_config.json')

#create bert model & get tr_input word embedding
model = modeling.BertModel(
    config=bert_config,
    is_training=True,
    input_ids = tf.convert_to_tensor(tr_inputs),
    input_mask=tf.convert_to_tensor(input_mask),
)
embedding = model.get_sequence_output() #[batch_size, seq_length, hidden_size]
print('input embedding:', embedding)
seq_length = embedding.shape[1].value
print('sequence length:',seq_length)
num_seq = embedding.shape[0].value
input_size = embedding.shape[2].value
#check pool_output
# model.get_pooled_output()

# get val word embedding
model = modeling.BertModel(
    config=bert_config,
    is_training=False,
    input_ids = tf.convert_to_tensor(val_inputs),
    input_mask=tf.convert_to_tensor(val_mask),
)
val_embedding = model.get_sequence_output() #[batch_size, seq_length, hidden_size]
print('val_embedding', val_embedding)
val_seq_num = val_embedding.shape[0]

# get test word embedding
model = modeling.BertModel(
    config=bert_config,
    is_training=False,
    input_ids = tf.convert_to_tensor(test_inputs),
    input_mask=tf.convert_to_tensor(test_mask),
)
test_embedding = model.get_sequence_output() #[batch_size, seq_length, hidden_size]
print('test_embedding', test_embedding)
test_seq_num = val_embedding.shape[0]

inputs = keras.Input(shape=(seq_length,input_size))
# inputs = keras.Input(shape=(156, 768))
training = GRU(10,return_sequences=True,stateful=False)(inputs)
dense1 = layers.Dense(100, activation='relu')(training)
# use mse loss
# output = layers.Dense(1, activation='relu')(dense1)
# use sparse loss
output = layers.Dense(20, activation='softmax')(dense1)
keras_model = keras.Model(inputs=inputs, outputs=output)
keras_model.summary()
# use mse loss
# keras_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])
#use sparse and sgd
# sgd = tf.keras.optimizers.SGD(learning_rate=0.0001, momentum=0.0, nesterov=False, name='SGD')
loss = tf.keras.losses.SparseCategoricalCrossentropy() #'categorical_accuracy'
keras_model.compile(loss=loss, optimizer='adam', metrics=['accuracy'])
tr_ouputs = tf.reshape(tr_ouputs, [num_seq,seq_length,1]) #change tr_ouputs to 3 dimension to feed in model
val_ouputs = tf.reshape(val_ouputs, [val_seq_num, seq_length,1])
val_ouputs  #<tf.Tensor 'Reshape_3:0' shape=(46, 8, 1) dtype=int64>
session = tf.InteractiveSession()
session.run(tf.global_variables_initializer())

history = keras_model.fit(embedding/255, tr_ouputs,
                    epochs=20,
                    steps_per_epoch=20 , validation_data = (val_embedding, val_ouputs), validation_steps=20)

import h5py
keras_model.save('model.h5')

history_dict = history.history
acc = history_dict['acc']
val_acc = history_dict['val_acc']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

fig, ax = plt.subplots()
ax.plot(acc)
ax.plot(val_acc)
ax.savefig('acc.png')

"""Make prediction"""

# id2tag = {tag2id[key]:key for key in tag2id}
# id2tag[0]='miss'
# id2tag
#
# predict1 = keras_model.predict(embedding, steps=1)
# predict1.shape
# predict1 = predict1.reshape(num_seq, seq_length)
# predict1 = np.rint(predict1)
# predict1[0]
#
# predict2 = []
# for i in range(test_seq_num):
#   temp = real_length[i]
#   predict2.append(predict1[i][:temp])
# predict2[:3]
#
# predict = [[id2tag[i] for i in temp] for temp in predict1]
# predict[0]
#
# # input sentence, output sentence and corresponding tags
# def ner_task(s, max_len, bert_config, model_path, id2tag):
#   # with tf.Session() as sess:
#   length_s = len(s)
#   s_token=[]
#   for temp in s.split():
#     for i in tokenizer.tokenize(temp):
#       s_token.append(i)
#   s_token = ['[CLS]']+s_token+['[SEP]']
#   s_id=[]
#   for i in s_token:
#     s_id+=tokenizer.convert_tokens_to_ids(i)
#   pad_s = s_id+[0]*(max_len-len(s_id)) if max_len>len(s_id) else s_id[:max_len]
#   pad_mask = [0 if i==0 else 1 for i in pad_s]
#
#   pad_s = tf.reshape(pad_s, [1,52])
#
#   pad_mask = tf.reshape(pad_mask, [1,52])
#
#   model = modeling.BertModel(
#     config=bert_config,
#     is_training=False,
#     input_ids = pad_s,
#     input_mask=pad_mask)
#   embedding = model.get_sequence_output() #[batch_size, seq_length, hidden_size]
#   kmodel = tf.keras.models.load_model(model_path)
#   predict1 = keras_model.predict(embedding, steps=1)
#   predict1 = np.rint(predict1)
#   predict = [id2tag[i[0]] for i in predict1[0]]
#   return predict
# temp = ['Thousands', 'of', 'demonstrators', 'have', 'marched', 'through', 'London', 'to', \
#      'protest', 'the', 'war', 'in', 'Iraq', 'and', 'demand', 'the', 'withdrawal', 'of', 'British', 'troops', 'from', 'that', 'country', '.']
# s=''
# for i in temp:
#   s+=' '+i
# # ner_task(s,max_len,bert_config,'model.h5',id2tag)
